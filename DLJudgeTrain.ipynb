{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a9fba566-2a33-4388-9107-da3518bb5135",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, WeightedRandomSampler\n",
    "from pytorch_tcn import TCN\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "a29116f4-e946-40c9-8099-9a68394f2015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fd76020c-d0de-4862-b48c-c67544761e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Video:\n",
    "    # Class to load and process a video given its ID\n",
    "    def __init__(self, video_id):\n",
    "        self.video_id = video_id\n",
    "        self.data = torch.load(\"all_videos.pt\", map_location=device, weights_only=True)[video_id-1] # [200, 17, 2]\n",
    "        self.data = self.data.view(200, -1).transpose(0, 1) # -> (200, 34) -> (34, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "73373b6b-e409-4aba-b61c-8c7bbb4dea55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset\n",
    "class DeadliftVideoDataset(Dataset):\n",
    "    def __init__(self, video_ids, labels_csv_path, transform=None):\n",
    "        self.video_ids = video_ids # List of video IDs\n",
    "        self.labels_dict = get_labels_from_csv(video_ids, labels_csv_path)\n",
    "        self.transform = transform # Optional transform to be applied to the video tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        vid = self.video_ids[idx]\n",
    "        video_instance = Video(vid)\n",
    "        data = video_instance.data  # Expected shape: [200, 17, 2]\n",
    "        if data is None:\n",
    "            raise ValueError(f\"Data for video {vid} could not be loaded.\")\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "        labels = self.labels_dict[vid]\n",
    "        return data, labels, vid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "dd46078e-90ab-4bd0-96b1-1fa4ec4160cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definition\n",
    "class DeadliftTCN(nn.Module):\n",
    "    def __init__(self, input_size, num_channels, num_classes, kernel_size=2, dropout=0.2):\n",
    "        super(DeadliftTCN, self).__init__()\n",
    "        self.tcn = TCN(input_size, num_channels, kernel_size=kernel_size, dropout=dropout)\n",
    "        self.fc = nn.Linear(num_channels[-1], num_classes)\n",
    "\n",
    "    def forward(self, x): # x shape: (batch_size, input_size, sequence_length)\n",
    "        y = self.tcn(x)  # Shape: (batch, num_channels[-1], sequence_length)\n",
    "        y = torch.mean(y, dim=2)  # Global average pooling over time dimension\n",
    "        logits = self.fc(y)       # Raw logits (BCEWithLogitsLoss expects raw logits)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f6a45d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels_from_csv(video_ids, csv_path=\"DLdataset/simple_labels.csv\"):\n",
    "    # Creates a dictionary mapping video IDs to their corresponding label tensors + duplicates for flips\n",
    "    labels_csv = pd.read_csv(csv_path).set_index(\"ID\") # Read the CSV and set 'ID' as the index\n",
    "    labels_dict = {}\n",
    "    for vid in video_ids:\n",
    "        try:\n",
    "            # Get the label values, convert them to float, and create a tensor\n",
    "            label_values = labels_csv.loc[vid].values.astype(float)\n",
    "            labels_dict[vid] = torch.tensor(label_values, dtype=torch.float32)\n",
    "        except Exception as e:\n",
    "            print(f\"Label for video {vid} not found or error occurred: {e}\")\n",
    "    \n",
    "    n = len(labels_dict) # Determine the number of entries\n",
    "    duplicate_dict = {vid + n: label for vid, label in labels_dict.items()} # Create duplicate dictionary with shifted index (for flipped vids)\n",
    "    combined = {} # Combine the original and duplicate dictionaries\n",
    "    combined.update(labels_dict)\n",
    "    combined.update(duplicate_dict)\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "df95ca0f-2c63-4401-b66b-13b680a12e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility Functions\n",
    "def compute_sample_weights(dataset):\n",
    "    # Computes sample weights to balance positive and negative classes\n",
    "    annotations = []\n",
    "    for vid in dataset.video_ids:\n",
    "        labels = dataset.labels_dict.get(vid, None)\n",
    "        if labels is not None:\n",
    "            annotations.append(1 if labels.sum() > 0 else 0)\n",
    "        else:\n",
    "            annotations.append(0)\n",
    "    annotations = pd.Series(annotations)\n",
    "    num_pos = annotations.sum()\n",
    "    num_neg = len(annotations) - num_pos\n",
    "    weights = []\n",
    "    for is_positive in annotations:\n",
    "        if is_positive:\n",
    "            weight = len(annotations) / (2.0 * num_pos)\n",
    "        else:\n",
    "            weight = len(annotations) / (2.0 * num_neg)\n",
    "        weights.append(weight)\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f62927bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pos_weight(labels_csv_path):\n",
    "    # Compute pos_weight for BCEWithLogitsLoss from the CSV file\n",
    "    df = pd.read_csv(labels_csv_path)\n",
    "    pos_weight_list = []\n",
    "    for col in ['Red', 'Blue', 'Yellow']:\n",
    "        pos_count = df[col].sum()\n",
    "        neg_count = len(df) - pos_count\n",
    "        ratio = neg_count / pos_count if pos_count > 0 else 1.0\n",
    "        pos_weight_list.append(ratio)\n",
    "    return torch.tensor(pos_weight_list, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "dfd47001",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(video_ids, labels_csv_path, batch_size=16, test_size=0.08):\n",
    "    # Create training and validation DataLoaders with a WeightedRandomSampler for the training set\n",
    "    dataset = DeadliftVideoDataset(video_ids, labels_csv_path)\n",
    "    indices = list(range(len(dataset)))\n",
    "    train_indices, val_indices = train_test_split(indices, test_size=test_size, random_state=18)\n",
    "    train_subset = Subset(dataset, train_indices)\n",
    "    val_subset = Subset(dataset, val_indices)\n",
    "    sample_weights = compute_sample_weights(dataset) # Compute sample weights only for the training set\n",
    "    train_weights = [sample_weights[i] for i in train_indices]\n",
    "    train_sampler = WeightedRandomSampler(train_weights, num_samples=len(train_weights), replacement=True)\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, sampler=train_sampler)\n",
    "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, val_loader, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b485a9b3-40de-4762-a020-3199e18ba531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=10):\n",
    "    # Train the model and evaluate it after each epoch.\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels, vids in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)  # Raw logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Training Loss: {epoch_loss:.4f}\")\n",
    "        #evaluate_model(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c5d483c4-87e4-44c4-a0a4-f38994dd41df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge_score(labels, predictions):\n",
    "        # For each sample, sum the three values.\n",
    "        pred_goodlift = (predictions.sum(dim=1) == 0) # If the sum is 0, it's a goodlift\n",
    "        true_goodlift = (labels.sum(dim=1) == 0)\n",
    "        misclassified = (pred_goodlift != true_goodlift).float().mean().item() # misclassification when the prediction doesn't match the label.\n",
    "        return (1.0 - misclassified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c085264b-733c-470d-84c4-50e554d4167b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Function\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    # Evaluate the model on the given data_loader and return metrics\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, vids in data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            # Apply sigmoid to convert logits to probabilities\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = (probs > 0.9).float()\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_labels.append(labels.cpu())\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    judge = judge_score(all_labels, all_preds)\n",
    "    print(f\"Evaluation Metrics -> Acc: {acc:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f} | F1: {f1:.4f}\")\n",
    "    print(f\"Evaluation Metrics -> Judge Score: {judge:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "aabb663d-16a7-447b-a70e-6a54429f4203",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_video(video_id, model, device, video_file=\"all_videos.pt\", threshold=0.9):\n",
    "    #Given a video ID, this function loads the preprocessed video data,\n",
    "    all_videos = torch.load(video_file, map_location=device) # Load the stacked tensor of all videos (shape: (N, 200, 17, 2))\n",
    "    video_tensor = all_videos[video_id - 1]  # shape: (200, 17, 2)\n",
    "    video_tensor = video_tensor.view(200, -1).transpose(0, 1)  # shape: (34, 200)\n",
    "    video_tensor = video_tensor.unsqueeze(0).to(device) # Add a batch dimension: final shape (1, 34, 200)\n",
    "    model.eval() # Set model to eval mode\n",
    "    with torch.no_grad():\n",
    "        logits = model(video_tensor)  # raw logits output\n",
    "        probs = torch.sigmoid(logits) # convert to probabilities\n",
    "        prediction = (probs > threshold).float() # Convert probabilities to binary predictions using the threshold\n",
    "    return prediction.cpu().tolist() # Return the prediction as a Python list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6b660ae7-3fc9-44d2-aa74-584bc390f14a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pos weight: tensor([11.5000, 13.4231,  2.1513], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Initialize model and hyperparameters\n",
    "labels_csv_path = \"DLdataset/simple_labels.csv\"\n",
    "video_ids = pd.read_csv(labels_csv_path)[\"ID\"].tolist()\n",
    "batch_size = 64\n",
    "train_loader, val_loader, dataset = create_dataloaders(video_ids, labels_csv_path, batch_size=batch_size) # Create DataLoaders\n",
    "pos_weight_tensor = compute_pos_weight(labels_csv_path).to(device) # Compute pos_weight for loss balancing\n",
    "print(f\"Pos weight: {pos_weight_tensor}\")\n",
    "\n",
    "# Model hyperparameters\n",
    "input_size = 34 # 17 joints * 2 coordinates per frame, flattened\n",
    "num_classes = 3 # 3 infractions\n",
    "num_channels = [50,75,100,125,125] # Filters per layer\n",
    "kernel_size = 7\n",
    "dropout = 0.5\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = DeadliftTCN(input_size, num_channels, num_classes, kernel_size, dropout).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6b7bb1-d07e-4c03-b5e6-5ec737017bbd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the model (evaluation happens inside the training loop via evaluate_model)\n",
    "num_epochs = 400\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, device, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8056bbfb-0dd2-4c55-8235-c13def88983a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"model_weights.pt\", map_location=device, weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "33dd2aa2-95ec-4779-a63c-8fc07ac5525e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics -> Acc: 0.8000 | Precision: 0.7418 | Recall: 0.9231 | F1: 0.8215\n",
      "Evaluation Metrics -> Judge Score: 0.8333\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(model, val_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655032e8-92cd-436d-84e5-9218c02e38fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dac0f84-e2e1-49c2-9bf3-cdf1837e4c5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
